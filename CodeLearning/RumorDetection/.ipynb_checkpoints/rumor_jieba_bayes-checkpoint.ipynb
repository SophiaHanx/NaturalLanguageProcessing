{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.读取数据，分词，去除停用词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1读取数据\n",
    "* weibo：DataFrame存储的博文及其对应标签；\n",
    "* content：list，元素为各个博文，逗号分隔，博文为字符串形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_not_rumor</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2013年3月5日，《明报》做了题目为《如果有来生，你愿不愿意再做中国人？》的投票调查，截止...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>各位同学，有认识住在华昌路靠近中兴路的海伦新苑的朋友吗？转给他们看一下好伐……帮在下留意一下...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>#北京曝光# @倍儿小爽 ：今天回家路上接到老妈电话，老妈冒雨给我送饭，拿到饭我让老妈打车回...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>一位可怜的94岁老奶奶，老伴去世，儿子伤寒死了，两个孙子在外面打工，她每天依靠捡垃圾为生，每...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>有人好奇黑人妹子是如何化妆的么？LZ带图详解 [哈哈] 太搞笑了，一直以为他们是不化妆的～～（转）</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_not_rumor                                            content\n",
       "0             0  2013年3月5日，《明报》做了题目为《如果有来生，你愿不愿意再做中国人？》的投票调查，截止...\n",
       "1             1  各位同学，有认识住在华昌路靠近中兴路的海伦新苑的朋友吗？转给他们看一下好伐……帮在下留意一下...\n",
       "2             1  #北京曝光# @倍儿小爽 ：今天回家路上接到老妈电话，老妈冒雨给我送饭，拿到饭我让老妈打车回...\n",
       "3             0  一位可怜的94岁老奶奶，老伴去世，儿子伤寒死了，两个孙子在外面打工，她每天依靠捡垃圾为生，每...\n",
       "4             1  有人好奇黑人妹子是如何化妆的么？LZ带图详解 [哈哈] 太搞笑了，一直以为他们是不化妆的～～（转）"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weibo = pd.read_csv('./data/all_data.txt',sep='\\t', names=['is_not_rumor','content'],encoding='utf-8')\n",
    "weibo = weibo.dropna()#删除缺失值\n",
    "weibo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3387, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weibo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将DataFrame中的Series转换为list\n",
    "content = weibo.content.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['一位可怜的94岁老奶奶，老伴去世，儿子伤寒死了，两个孙子在外面打工，她每天依靠捡垃圾为生，每天捡垃圾捡到凌晨2点，也只能转5、6元。奶奶双腿已经裂开，因为没钱，一直没有医治。求扩散~~每转一条微博，腾讯公益就像老奶奶捐出1毛钱，多转几次吧，不会脏了你微博，对么？', '有人好奇黑人妹子是如何化妆的么？LZ带图详解 [哈哈] 太搞笑了，一直以为他们是不化妆的～～（转）']\n"
     ]
    }
   ],
   "source": [
    "print (content[3:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2分词\n",
    "使用jieba分词器进行分词\n",
    "* content_S：list of list，各个博文的分词结果；\n",
    "* df_content：将content_S转换为DataFrame格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\dell\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.867 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "#其元素为所有博文的分词结果，list类型\n",
    "content_S = []\n",
    "for line in content:\n",
    "    current_segment = jieba.lcut(line)#列表，元素为分割出来的词\n",
    "    if len(current_segment) > 1 and current_segment != '\\r\\n': #换行符\n",
    "        content_S.append(current_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['有人',\n",
       " '好奇',\n",
       " '黑人',\n",
       " '妹子',\n",
       " '是',\n",
       " '如何',\n",
       " '化妆',\n",
       " '的',\n",
       " '么',\n",
       " '？',\n",
       " 'LZ',\n",
       " '带图',\n",
       " '详解',\n",
       " ' ',\n",
       " '[',\n",
       " '哈哈',\n",
       " ']',\n",
       " ' ',\n",
       " '太',\n",
       " '搞笑',\n",
       " '了',\n",
       " '，',\n",
       " '一直',\n",
       " '以为',\n",
       " '他们',\n",
       " '是',\n",
       " '不',\n",
       " '化妆',\n",
       " '的',\n",
       " '～',\n",
       " '～',\n",
       " '（',\n",
       " '转',\n",
       " '）']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_S[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[2013, 年, 3, 月, 5, 日, ，, 《, 明报, 》, 做, 了, 题目, 为...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[各位, 同学, ，, 有, 认识, 住, 在, 华昌路, 靠近, 中兴路, 的, 海伦, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[#, 北京, 曝光, #,  , @, 倍儿, 小爽,  , ：, 今天, 回家路上, 接...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[一位, 可怜, 的, 94, 岁, 老奶奶, ，, 老伴, 去世, ，, 儿子, 伤寒, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[有人, 好奇, 黑人, 妹子, 是, 如何, 化妆, 的, 么, ？, LZ, 带图, 详...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           content_S\n",
       "0  [2013, 年, 3, 月, 5, 日, ，, 《, 明报, 》, 做, 了, 题目, 为...\n",
       "1  [各位, 同学, ，, 有, 认识, 住, 在, 华昌路, 靠近, 中兴路, 的, 海伦, ...\n",
       "2  [#, 北京, 曝光, #,  , @, 倍儿, 小爽,  , ：, 今天, 回家路上, 接...\n",
       "3  [一位, 可怜, 的, 94, 岁, 老奶奶, ，, 老伴, 去世, ，, 儿子, 伤寒, ...\n",
       "4  [有人, 好奇, 黑人, 妹子, 是, 如何, 化妆, 的, 么, ？, LZ, 带图, 详..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content=pd.DataFrame({'content_S':content_S})\n",
    "df_content.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3去除停用词\n",
    "* contents_clean：list of list，contents去除停用词之后的结果；\n",
    "* all_words：所有的词，包含重复值；\n",
    "* df_content_clean：DtaFrame格式的contents_clean；\n",
    "* df_all_words：DtaFrame格式的all_words。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stopword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stopword\n",
       "0        !\n",
       "1        \"\n",
       "2        #\n",
       "3        $\n",
       "4        %"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords=pd.read_csv(\"./stopwords/stopwords.txt\",index_col=False,sep=\"\\t\",quoting=3,names=['stopword'], encoding='utf-8')\n",
    "stopwords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_stopwords(contents,stopwords):\n",
    "    contents_clean = []\n",
    "    all_words = []\n",
    "    for line in contents:\n",
    "        line_clean = []\n",
    "        for word in line:\n",
    "            if word in stopwords:\n",
    "                continue\n",
    "            line_clean.append(word)\n",
    "            all_words.append(str(word))#str()转换为字符串##记录所有line_clean中的词\n",
    "        contents_clean.append(line_clean)\n",
    "    return contents_clean,all_words\n",
    "    #print (contents_clean)\n",
    "        \n",
    "\n",
    "contents = df_content.content_S.values.tolist()    \n",
    "stopwords = stopwords.stopword.values.tolist()\n",
    "contents_clean,all_words = drop_stopwords(contents,stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contents_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[2013, 明报, 做, 题目, 有来生, 愿不愿意, 做, 中国, 投票, 调查, 截止...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[同学, 住, 华昌路, 靠近, 中兴路, 海伦, 新苑, 朋友, 转给, 好伐, 帮, 留...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[北京, 曝光,  , 倍儿, 小爽,  , 回家路上, 接到, 老妈, 电话, 老妈, 冒...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[一位, 可怜, 岁, 老奶奶, 老伴, 去世, 儿子, 伤寒, 死, 两个, 孙子, 外面...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[有人, 好奇, 黑人, 妹子, 化妆, LZ, 带图, 详解,  ,  , 太, 搞笑, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      contents_clean\n",
       "0  [2013, 明报, 做, 题目, 有来生, 愿不愿意, 做, 中国, 投票, 调查, 截止...\n",
       "1  [同学, 住, 华昌路, 靠近, 中兴路, 海伦, 新苑, 朋友, 转给, 好伐, 帮, 留...\n",
       "2  [北京, 曝光,  , 倍儿, 小爽,  , 回家路上, 接到, 老妈, 电话, 老妈, 冒...\n",
       "3  [一位, 可怜, 岁, 老奶奶, 老伴, 去世, 儿子, 伤寒, 死, 两个, 孙子, 外面...\n",
       "4  [有人, 好奇, 黑人, 妹子, 化妆, LZ, 带图, 详解,  ,  , 太, 搞笑, ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content_clean=pd.DataFrame({'contents_clean':contents_clean})\n",
    "df_content_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>明报</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>做</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>题目</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>有来生</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  all_words\n",
       "0      2013\n",
       "1        明报\n",
       "2         做\n",
       "3        题目\n",
       "4       有来生"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_words=pd.DataFrame({'all_words':all_words})\n",
    "df_all_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4统计词频，绘制词云图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count=df_all_words\n",
    "words_count['count']=df_all_words['all_words'].apply(lambda x: dict(df_all_words['all_words'].value_counts())[x])\n",
    "words_count.drop_duplicates([\"all_words\"],inplace=True)\n",
    "words_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 5.0)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud=WordCloud(font_path=\"./data/simhei.ttf\",background_color=\"white\",max_font_size=80)\n",
    "word_frequence = {x[0]:x[1] for x in words_count.head(100).values}\n",
    "wordcloud=wordcloud.fit_words(word_frequence)\n",
    "plt.imshow(wordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.分类检测\n",
    "贝叶斯分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1划分数据集，数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contents_clean</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3382</th>\n",
       "      <td>[作品, 全职, 高手, 发布, 新, 章节, 一千六百, 三章,  , 爱, 力量, 本书...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3383</th>\n",
       "      <td>[微博报, 讯, 东方, 早报, 报道, 财报, 显示, 南方, 报业, 传媒, 集团, 一...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3384</th>\n",
       "      <td>[天津, 北京, 山东, 江苏, 发出, 警示, 请, 传出去,  , 隐翅虫, 青腰, 虫...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3385</th>\n",
       "      <td>[俄媒, 如受, 制裁,  , 俄罗斯, 立法, 没收, 欧美, 企业, 俄, 资产, 吃惊...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3386</th>\n",
       "      <td>[盘锦市, 公安局, 查, 村民, 王树杰, 警察, 张研, 开枪, 杀害, 调查结果, 却...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         contents_clean  label\n",
       "3382  [作品, 全职, 高手, 发布, 新, 章节, 一千六百, 三章,  , 爱, 力量, 本书...      1\n",
       "3383  [微博报, 讯, 东方, 早报, 报道, 财报, 显示, 南方, 报业, 传媒, 集团, 一...      1\n",
       "3384  [天津, 北京, 山东, 江苏, 发出, 警示, 请, 传出去,  , 隐翅虫, 青腰, 虫...      0\n",
       "3385  [俄媒, 如受, 制裁,  , 俄罗斯, 立法, 没收, 欧美, 企业, 俄, 资产, 吃惊...      1\n",
       "3386  [盘锦市, 公安局, 查, 村民, 王树杰, 警察, 张研, 开枪, 杀害, 调查结果, 却...      0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=pd.DataFrame({'contents_clean':contents_clean,'label':weibo['is_not_rumor']})\n",
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 划分数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_train['contents_clean'].values, df_train['label'].values, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['泰森',\n",
       " '赌城',\n",
       " '讲',\n",
       " '相声',\n",
       " '遭热',\n",
       " '捧',\n",
       " '法新社',\n",
       " '报道',\n",
       " '岁',\n",
       " '前',\n",
       " '世界',\n",
       " '拳王',\n",
       " '泰森',\n",
       " '昨晚',\n",
       " '拉斯维加斯',\n",
       " '米高梅',\n",
       " '酒店',\n",
       " '剧场',\n",
       " '开启',\n",
       " '单口相声',\n",
       " '演艺',\n",
       " '生涯',\n",
       " '历时',\n",
       " '两',\n",
       " '小时',\n",
       " '首演',\n",
       " '中',\n",
       " '泰森以',\n",
       " '夸张',\n",
       " '口吻',\n",
       " '噱头',\n",
       " '百出',\n",
       " '讲述',\n",
       " '穷小子',\n",
       " '拳王',\n",
       " '强奸犯',\n",
       " '直至',\n",
       " '一贫如洗',\n",
       " '经历',\n",
       " '现场',\n",
       " '观众',\n",
       " '对泰森',\n",
       " '报以',\n",
       " '热',\n",
       " '捧',\n",
       " '称其',\n",
       " '教育',\n",
       " '程度',\n",
       " '有限',\n",
       " '极具',\n",
       " '讲演',\n",
       " '天赋']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'泰森 赌城 讲 相声 遭热 捧 法新社 报道 岁 前 世界 拳王 泰森 昨晚 拉斯维加斯 米高梅 酒店 剧场 开启 单口相声 演艺 生涯 历时 两 小时 首演 中 泰森以 夸张 口吻 噱头 百出 讲述 穷小子 拳王 强奸犯 直至 一贫如洗 经历 现场 观众 对泰森 报以 热 捧 称其 教育 程度 有限 极具 讲演 天赋'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "for line_index in range(len(x_train)):\n",
    "    try:\n",
    "        #x_train[line_index][word_index] = str(x_train[line_index][word_index])\n",
    "        words.append(' '.join(x_train[line_index]))\n",
    "    except:\n",
    "        print (line_index,word_index)\n",
    "words[-1]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2贝叶斯分类器分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用CountVectorizer作词嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(lowercase=False, max_features=4000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(analyzer='word', max_features=4000,  lowercase = False)\n",
    "#为words中每个词建立字典对应\n",
    "vec.fit(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = MultinomialNB()\n",
    "classifier.fit(vec.transform(words), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'晚安 心   Bernadette s   Song 小伙伴 生活 爆炸 最新 Howard 写给 Bernadette 那首 If   I   Didn t   Have   You 戳 泪点 首歌 作者 客串 Raj 前女友 Lucy Kate   Micucci   写 小伙伴 请 才华 鼓掌 欣赏 Bernadette s   Song http t cn zR6vrLS'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试集数据处理\n",
    "test_words = []\n",
    "for line_index in range(len(x_test)):\n",
    "    try:\n",
    "        #x_train[line_index][word_index] = str(x_train[line_index][word_index])\n",
    "        test_words.append(' '.join(x_test[line_index]))\n",
    "    except:\n",
    "         print (line_index,word_index)\n",
    "test_words[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8630460448642266"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(vec.transform(test_words), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用TfidfVectorizer作词嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(lowercase=False, max_features=4000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer='word', max_features=4000,  lowercase = False)\n",
    "vectorizer.fit(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier1 = MultinomialNB()\n",
    "classifier1.fit(vectorizer.transform(words), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8748524203069658"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier1.score(vectorizer.transform(test_words), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TF2.1]",
   "language": "python",
   "name": "conda-env-TF2.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
