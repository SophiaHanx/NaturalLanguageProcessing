{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "# 魔法命令，使用后画图不用show了\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re# 引入正则\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.解压词向量并加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1解压词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2# 用来解压文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./embeddings/sgns.weibo.bigram\", 'wb') as new_file, open(\"./embeddings/sgns.weibo.bigram.bz2\", 'rb') as file:\n",
    "    decompressor = bz2.BZ2Decompressor()\n",
    "    for data in iter(lambda : file.read(100 * 1024), b''):\n",
    "        new_file.write(decompressor.decompress(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2加载词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors# gensim用来加载预训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_model = KeyedVectors.load_word2vec_format('./embeddings/sgns.weibo.bigram', \n",
    "                                             binary=False,\n",
    "                                             unicode_errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.语料预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1读取原始文本\n",
    "* weibo：DataFrame存储的博文及其对应标签\n",
    "* content：list存储的原始文本字符串\n",
    "* label：标签，1为非谣言"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_not_rumor</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2013年3月5日，《明报》做了题目为《如果有来生，你愿不愿意再做中国人？》的投票调查，截止...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>各位同学，有认识住在华昌路靠近中兴路的海伦新苑的朋友吗？转给他们看一下好伐……帮在下留意一下...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>#北京曝光# @倍儿小爽 ：今天回家路上接到老妈电话，老妈冒雨给我送饭，拿到饭我让老妈打车回...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>一位可怜的94岁老奶奶，老伴去世，儿子伤寒死了，两个孙子在外面打工，她每天依靠捡垃圾为生，每...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>有人好奇黑人妹子是如何化妆的么？LZ带图详解 [哈哈] 太搞笑了，一直以为他们是不化妆的～～（转）</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_not_rumor                                            content\n",
       "0             0  2013年3月5日，《明报》做了题目为《如果有来生，你愿不愿意再做中国人？》的投票调查，截止...\n",
       "1             1  各位同学，有认识住在华昌路靠近中兴路的海伦新苑的朋友吗？转给他们看一下好伐……帮在下留意一下...\n",
       "2             1  #北京曝光# @倍儿小爽 ：今天回家路上接到老妈电话，老妈冒雨给我送饭，拿到饭我让老妈打车回...\n",
       "3             0  一位可怜的94岁老奶奶，老伴去世，儿子伤寒死了，两个孙子在外面打工，她每天依靠捡垃圾为生，每...\n",
       "4             1  有人好奇黑人妹子是如何化妆的么？LZ带图详解 [哈哈] 太搞笑了，一直以为他们是不化妆的～～（转）"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weibo = pd.read_csv('./data/all_data.txt',sep='\\t', names=['is_not_rumor','content'],encoding='utf-8')\n",
    "weibo = weibo.dropna()#删除缺失值\n",
    "weibo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3387, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weibo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将DataFrame中的Series转换为list\n",
    "content = weibo.content.values.tolist()\n",
    "label=weibo.is_not_rumor.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['一位可怜的94岁老奶奶，老伴去世，儿子伤寒死了，两个孙子在外面打工，她每天依靠捡垃圾为生，每天捡垃圾捡到凌晨2点，也只能转5、6元。奶奶双腿已经裂开，因为没钱，一直没有医治。求扩散~~每转一条微博，腾讯公益就像老奶奶捐出1毛钱，多转几次吧，不会脏了你微博，对么？', '有人好奇黑人妹子是如何化妆的么？LZ带图详解 [哈哈] 太搞笑了，一直以为他们是不化妆的～～（转）']\n"
     ]
    }
   ],
   "source": [
    "print (content[3:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2进行分词和tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/lancopku/PKUSeg-python\n",
    "\n",
    "对每一条微博文本text，\n",
    "1. 去掉每个样本的标点符号；\n",
    "2. 用pkuseg分词，得到存放分词结果的cut_list；\n",
    "3. 去掉cut_list中的停用词得到cut_list_clean；\n",
    "3. 将分词结果cut_list_clean索引化（使用北京师范大学中文信息处理研究所与中国人民大学 DBIIR 实验室的研究者开源的\"chinese-word-vectors\"），这样每一例评价的文本变成一段索引数字，对应着预训练词向量模型中的词。\n",
    "\n",
    "将每个text的结果存到train_tokens中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkuseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入停用词\n",
    "stopwords=pd.read_csv(\"./stopwords/stopwords.txt\",index_col=False,sep=\"\\t\",quoting=3,names=['stopword'], encoding='utf-8')\n",
    "stopwords = stopwords.stopword.values.tolist()#转为list形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/lancopku/pkuseg-python/releases/download/v0.0.16/web.zip\" to C:\\Users\\dell/.pkuseg\\web.zip\n",
      "100%|█████████████████████████| 17478354/17478354 [00:12<00:00, 1432747.33it/s]\n"
     ]
    }
   ],
   "source": [
    "seg = pkuseg.pkuseg(model_name='web')  # 程序会自动下载所对应的细领域模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = []\n",
    "for text in content:\n",
    "    # 去掉标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # pkuseg分词\n",
    "    cut_list = seg.cut(text)\n",
    "\n",
    "    #去除停用词\n",
    "    cut_list_clean=[]\n",
    "    for word in cut_list:\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        cut_list_clean.append(word)\n",
    "    \n",
    "    #索引化\n",
    "    for i, word in enumerate(cut_list_clean): # enumerate()\n",
    "        try:\n",
    "            # 将词转换为索引index\n",
    "            cut_list_clean[i] = cn_model.vocab[word].index\n",
    "        except KeyError:\n",
    "            # 如果词不在字典中，则输出0\n",
    "            cut_list_clean[i] = 0\n",
    "    train_tokens.append(cut_list_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3索引长度标准化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为每段评语的长度是不一样的，如果单纯取最长的一个评语，并把其他评填充成同样的长度，这样十分浪费计算资源，所以取一个折衷的长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获得所有tokens的长度\n",
    "num_tokens = [len(tokens) for tokens in train_tokens]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 取tokens平均值并加上两个tokens的标准差，\n",
    "# 假设tokens长度的分布为正态分布，则max_tokens这个值可以涵盖95%左右的样本\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### padding（填充）和truncating（修剪）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们把文本转换为tokens（索引）之后，每一串索引的长度并不相等，所以为了方便模型的训练我们需要把索引的长度标准化，上面我们选择了max_tokens个可以涵盖95%训练样本的长度，接下来我们进行padding和truncating，我们一般采用'pre'的方法，这会在文本索引的前面填充0，因为根据一些研究资料中的实践，如果在文本索引后面填充0的话，会对模型造成一些不良影响。 \n",
    "\n",
    "进行padding和truncating， 输入的train_tokens是一个list\n",
    "返回的train_pad是一个numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pad = pad_sequences(train_tokens, maxlen=max_tokens,\n",
    "                            padding='pre', truncating='pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4准备Embedding Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 作为模型的输入，需要准备一个维度为 (𝑛𝑢𝑚𝑤𝑜𝑟𝑑𝑠,𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔𝑑𝑖𝑚) 的embedding矩阵，num words代表使用的词汇的数量。\n",
    "\n",
    "\n",
    "* 不进行词向量的训练，而是使用预训练的词向量——北京师范大学中文信息处理研究所与中国人民大学 DBIIR 实验室的研究者开源的\"chinese-word-vectors\"；https://github.com/Embedding/Chinese-Word-Vectors ；emdedding dimension在现在使用的预训练词向量模型中是300，每一个词汇都用一个长度为300的向量表示。\n",
    "\n",
    "\n",
    "* 注意只选择使用前50k个使用频率最高的词，在这个预训练词向量模型中，一共有260万词汇量，如果全部使用在分类问题上会很浪费计算资源，因为训练样本很小，如果有更多的训练样本时，在分类问题上可以考虑减少使用的词汇量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words = 50000\n",
    "embedding_dim=300\n",
    "# 初始化embedding_matrix，之后在keras上进行应用\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "# embedding_matrix为一个 [num_words，embedding_dim] 的矩阵\n",
    "# 维度为 50000 * 300\n",
    "for i in range(num_words):\n",
    "    embedding_matrix[i,:] = cn_model[cn_model.index2word[i]]#前50000个index对应的词的词向量\n",
    "embedding_matrix = embedding_matrix.astype('float32')\n",
    "# 检查index是否对应，\n",
    "# 输出300意义为长度为300的embedding向量一一对应\n",
    "np.sum(cn_model[cn_model.index2word[333]] == embedding_matrix[333] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超出五万个词向量的词用0代替\n",
    "train_pad[train_pad>=num_words ] = 0\n",
    "\n",
    "# 准备target向量，前2000样本为1，后2000为0\n",
    "train_target = np.array(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.训练语料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1划分训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行训练和测试样本的分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 90%的样本用来训练，剩余10%用来测试\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_pad,\n",
    "                                                    train_target,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2搭建网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 用keras搭建LSTM模型，模型的第一层是Embedding层，只有当我们把tokens索引转换为词向量矩阵之后，才可以用神经网络对文本进行处理。 keras提供了Embedding接口，避免了繁琐的稀疏矩阵操作。\n",
    "* 在Embedding层我们输入的矩阵为：(𝑏𝑎𝑡𝑐ℎ𝑠𝑖𝑧𝑒,𝑚𝑎𝑥𝑡𝑜𝑘𝑒𝑛𝑠)输出矩阵为:(𝑏𝑎𝑡𝑐ℎ𝑠𝑖𝑧𝑒,𝑚𝑎𝑥𝑡𝑜𝑘𝑒𝑛𝑠,𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔𝑑𝑖𝑚)。\n",
    "* 使用预训练的词向量，将trainable设为False，即不可训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_tokens,\n",
    "                    trainable=False))\n",
    "model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(units=32, return_sequences=False)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "optimizer=Adam(lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================model1=================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_tokens,\n",
    "                    trainable=False))\n",
    "model1.add(Bidirectional(GRU(32)))\n",
    "model1.add(Dense(6, activation='relu'))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "optimizer=Adam(lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3模型配置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型保存（断点续训）、early stoping、学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立一个权重的存储点\n",
    "checkpoint_save_path=\"./checkpoint/rumor_LSTM.ckpt\"\n",
    "if os.path.exists(checkpoint_save_path+'.index'):\n",
    "    print('----------load the model----------')\n",
    "    model.load_weights(checkpoint_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存参数和模型\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_save_path, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义early stoping如果3个epoch内validation loss没有改善则停止训练\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "# 自动降低learning rate\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1, min_lr=1e-8, patience=0,\n",
    "                                       verbose=1)\n",
    "# 定义callback函数\n",
    "callbacks = [\n",
    "    earlystopping, \n",
    "#    checkpoint,\n",
    "    lr_reduction\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================model1==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立一个权重的存储点\n",
    "checkpoint_save_path1=\"./checkpoint/rumor_GRU.ckpt\"\n",
    "if os.path.exists(checkpoint_save_path1+'.index'):\n",
    "    print('----------load the model----------')\n",
    "    model1.load_weights(checkpoint_save_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存参数和模型\n",
    "checkpoint1 = ModelCheckpoint(filepath=checkpoint_save_path1, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2743 samples, validate on 305 samples\n",
      "Epoch 1/20\n",
      "2743/2743 [==============================] - 69s 25ms/sample - loss: 0.6390 - accuracy: 0.6482 - val_loss: 0.4600 - val_accuracy: 0.8295\n",
      "Epoch 2/20\n",
      "2688/2743 [============================>.] - ETA: 0s - loss: 0.4470 - accuracy: 0.8039\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "2743/2743 [==============================] - 14s 5ms/sample - loss: 0.4463 - accuracy: 0.8046 - val_loss: 0.4779 - val_accuracy: 0.8000\n",
      "Epoch 3/20\n",
      "2743/2743 [==============================] - 11s 4ms/sample - loss: 0.4171 - accuracy: 0.8181 - val_loss: 0.3556 - val_accuracy: 0.8426\n",
      "Epoch 4/20\n",
      "2743/2743 [==============================] - 11s 4ms/sample - loss: 0.3767 - accuracy: 0.8400 - val_loss: 0.3397 - val_accuracy: 0.8623\n",
      "Epoch 5/20\n",
      "2743/2743 [==============================] - 10s 4ms/sample - loss: 0.3627 - accuracy: 0.8498 - val_loss: 0.3333 - val_accuracy: 0.8590\n",
      "Epoch 6/20\n",
      "2743/2743 [==============================] - 11s 4ms/sample - loss: 0.3522 - accuracy: 0.8600 - val_loss: 0.3276 - val_accuracy: 0.8721\n",
      "Epoch 7/20\n",
      "2743/2743 [==============================] - 12s 5ms/sample - loss: 0.3420 - accuracy: 0.8618 - val_loss: 0.3238 - val_accuracy: 0.8689\n",
      "Epoch 8/20\n",
      "2743/2743 [==============================] - 10s 4ms/sample - loss: 0.3340 - accuracy: 0.8647 - val_loss: 0.3211 - val_accuracy: 0.8754\n",
      "Epoch 9/20\n",
      "2743/2743 [==============================] - 11s 4ms/sample - loss: 0.3233 - accuracy: 0.8724 - val_loss: 0.3171 - val_accuracy: 0.8721\n",
      "Epoch 10/20\n",
      "2743/2743 [==============================] - 11s 4ms/sample - loss: 0.3132 - accuracy: 0.8746 - val_loss: 0.3117 - val_accuracy: 0.8787\n",
      "Epoch 11/20\n",
      "2688/2743 [============================>.] - ETA: 0s - loss: 0.3026 - accuracy: 0.8802\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "2743/2743 [==============================] - 11s 4ms/sample - loss: 0.3020 - accuracy: 0.8801 - val_loss: 0.3146 - val_accuracy: 0.8787\n",
      "Epoch 12/20\n",
      "2743/2743 [==============================] - 11s 4ms/sample - loss: 0.2927 - accuracy: 0.8859 - val_loss: 0.3076 - val_accuracy: 0.8852\n",
      "Epoch 13/20\n",
      "2688/2743 [============================>.] - ETA: 0s - loss: 0.2887 - accuracy: 0.8884\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "2743/2743 [==============================] - 10s 4ms/sample - loss: 0.2895 - accuracy: 0.8881 - val_loss: 0.3089 - val_accuracy: 0.8787\n",
      "Epoch 14/20\n",
      "2688/2743 [============================>.] - ETA: 0s - loss: 0.2898 - accuracy: 0.8873\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "2743/2743 [==============================] - 11s 4ms/sample - loss: 0.2888 - accuracy: 0.8877 - val_loss: 0.3088 - val_accuracy: 0.8787\n",
      "Epoch 15/20\n",
      "2688/2743 [============================>.] - ETA: 0s - loss: 0.2897 - accuracy: 0.8865\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "2743/2743 [==============================] - 11s 4ms/sample - loss: 0.2886 - accuracy: 0.8877 - val_loss: 0.3087 - val_accuracy: 0.8787\n",
      "Epoch 16/20\n",
      "2688/2743 [============================>.] - ETA: 0s - loss: 0.2860 - accuracy: 0.8888\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1e-08.\n",
      "2743/2743 [==============================] - 11s 4ms/sample - loss: 0.2886 - accuracy: 0.8877 - val_loss: 0.3087 - val_accuracy: 0.8787\n",
      "Epoch 17/20\n",
      "2743/2743 [==============================] - 10s 4ms/sample - loss: 0.2886 - accuracy: 0.8877 - val_loss: 0.3087 - val_accuracy: 0.8787\n",
      "Epoch 00017: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xcba3c901c8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,validation_split=0.1,epochs=20,batch_size=128,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================model1==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2743 samples, validate on 305 samples\n",
      "Epoch 1/20\n",
      "2743/2743 [==============================] - 29s 11ms/sample - loss: 0.7042 - accuracy: 0.5035 - val_loss: 0.6928 - val_accuracy: 0.5279\n",
      "Epoch 2/20\n",
      "2743/2743 [==============================] - 7s 2ms/sample - loss: 0.6920 - accuracy: 0.5472 - val_loss: 0.6910 - val_accuracy: 0.5279\n",
      "Epoch 3/20\n",
      "2743/2743 [==============================] - 4s 1ms/sample - loss: 0.6785 - accuracy: 0.5472 - val_loss: 0.6643 - val_accuracy: 0.5279\n",
      "Epoch 4/20\n",
      "2743/2743 [==============================] - 4s 1ms/sample - loss: 0.6348 - accuracy: 0.5472 - val_loss: 0.6109 - val_accuracy: 0.5279\n",
      "Epoch 5/20\n",
      "2743/2743 [==============================] - 4s 2ms/sample - loss: 0.5728 - accuracy: 0.5633 - val_loss: 0.5394 - val_accuracy: 0.7475\n",
      "Epoch 6/20\n",
      "2743/2743 [==============================] - 5s 2ms/sample - loss: 0.4963 - accuracy: 0.8064 - val_loss: 0.4369 - val_accuracy: 0.8459\n",
      "Epoch 7/20\n",
      "2743/2743 [==============================] - 4s 1ms/sample - loss: 0.4138 - accuracy: 0.8483 - val_loss: 0.3653 - val_accuracy: 0.8459\n",
      "Epoch 8/20\n",
      "2743/2743 [==============================] - 4s 1ms/sample - loss: 0.3249 - accuracy: 0.8684 - val_loss: 0.3333 - val_accuracy: 0.8689\n",
      "Epoch 9/20\n",
      "2743/2743 [==============================] - 4s 2ms/sample - loss: 0.2782 - accuracy: 0.8917 - val_loss: 0.3098 - val_accuracy: 0.8787\n",
      "Epoch 10/20\n",
      "2688/2743 [============================>.] - ETA: 0s - loss: 0.2398 - accuracy: 0.9129\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "2743/2743 [==============================] - 5s 2ms/sample - loss: 0.2404 - accuracy: 0.9129 - val_loss: 0.3311 - val_accuracy: 0.8656\n",
      "Epoch 11/20\n",
      "2743/2743 [==============================] - 4s 1ms/sample - loss: 0.2109 - accuracy: 0.9282 - val_loss: 0.3019 - val_accuracy: 0.8820\n",
      "Epoch 12/20\n",
      "2743/2743 [==============================] - 4s 1ms/sample - loss: 0.2021 - accuracy: 0.9304 - val_loss: 0.3012 - val_accuracy: 0.8787\n",
      "Epoch 13/20\n",
      "2688/2743 [============================>.] - ETA: 0s - loss: 0.1997 - accuracy: 0.9330\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "2743/2743 [==============================] - 3s 1ms/sample - loss: 0.1973 - accuracy: 0.9340 - val_loss: 0.3019 - val_accuracy: 0.8787\n",
      "Epoch 14/20\n",
      "2688/2743 [============================>.] - ETA: 0s - loss: 0.1948 - accuracy: 0.9334 ETA: 0s - loss: 0.1929 - accuracy: \n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "2743/2743 [==============================] - 4s 1ms/sample - loss: 0.1941 - accuracy: 0.9336 - val_loss: 0.3018 - val_accuracy: 0.8787\n",
      "Epoch 15/20\n",
      "2688/2743 [============================>.] - ETA: 0s - loss: 0.1955 - accuracy: 0.9330\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "2743/2743 [==============================] - 4s 1ms/sample - loss: 0.1937 - accuracy: 0.9340 - val_loss: 0.3017 - val_accuracy: 0.8787\n",
      "Epoch 16/20\n",
      "2688/2743 [============================>.] - ETA: 0s - loss: 0.1944 - accuracy: 0.9334\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "2743/2743 [==============================] - 4s 1ms/sample - loss: 0.1936 - accuracy: 0.9340 - val_loss: 0.3017 - val_accuracy: 0.8787\n",
      "Epoch 17/20\n",
      "2688/2743 [============================>.] - ETA: 0s - loss: 0.1939 - accuracy: 0.9338\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1e-08.\n",
      "2743/2743 [==============================] - 4s 1ms/sample - loss: 0.1936 - accuracy: 0.9340 - val_loss: 0.3017 - val_accuracy: 0.8787\n",
      "Epoch 00017: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xcbb67b17c8>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(X_train, y_train,validation_split=0.1,epochs=20,batch_size=128,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5应用于测试集\n",
    "首先对测试样本进行预测，得到了还算满意的准确度。之后我们定义一个预测函数，来预测输入的文本的极性，可见模型对于否定句和一些简单的逻辑结构都可以进行准确的判断。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339/339 [==============================] - 2s 5ms/sample - loss: 0.3997 - accuracy: 0.8289\n",
      "Accuracy:82.89%\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================model1==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339/339 [==============================] - 1s 3ms/sample - loss: 0.3591 - accuracy: 0.8614\n",
      "Accuracy:86.14%\n"
     ]
    }
   ],
   "source": [
    "result = model1.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6实例展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rumor_LSTM(text,label):\n",
    "    print(text)\n",
    "    # 去标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # 分词\n",
    "    cut = seg.cut(text)\n",
    "    #去除停用词\n",
    "    cut_clean=[]\n",
    "    for word in cut:\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        cut_clean.append(word)\n",
    "    # tokenize\n",
    "    for i, word in enumerate(cut_clean):\n",
    "        try:\n",
    "            cut_clean[i] = cn_model.vocab[word].index\n",
    "            if cut_clean[i] >= 50000:\n",
    "                cut_clean[i] = 0\n",
    "        except KeyError:\n",
    "            cut_clean[i] = 0\n",
    "    # padding\n",
    "    tokens_pad = pad_sequences([cut_clean], maxlen=max_tokens,\n",
    "                           padding='pre', truncating='pre')\n",
    "    # 预测\n",
    "    dic={0:'谣言',1:'非谣言'}\n",
    "    result = model.predict(x=tokens_pad)\n",
    "    coef = result[0][0]\n",
    "    if coef >= 0.5:\n",
    "        print('真实是'+dic[label],'预测是非谣言','output=%.2f'%coef)\n",
    "    else:\n",
    "        print('真实是'+dic[label],'预测是谣言','output=%.2f'%coef)\n",
    "    print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "兴仁县今天抢小孩没抢走，把孩子母亲捅了一刀，看见这车的注意了，真事，车牌号辽HFM055！！！！！赶紧散播！ 都别带孩子出去瞎转悠了 尤其别让老人自己带孩子出去 太危险了 注意了！！！！辽HFM055北京现代朗动，在各学校门口抢小孩！！！110已经 证实！！全市通缉！！\n",
      "真实是谣言 预测是谣言 output=0.10\n",
      "---------------------------------------------\n",
      "重庆真实新闻:2016年6月1日在重庆梁平县袁驿镇发生一起抢儿童事件，做案人三个中年男人，在三中学校到镇街上的一条小路上，把小孩直接弄晕(儿童是袁驿新幼儿园中班的一名学生)，正准备带走时被家长及时发现用棒子赶走了做案人，故此获救！请各位同胞们以此引起非常重视，希望大家有爱心的人传递下\n",
      "真实是谣言 预测是谣言 output=0.10\n",
      "---------------------------------------------\n",
      "@尾熊C 要提前预习育儿知识的话，建议看一些小巫写的书，嘻嘻\n",
      "真实是非谣言 预测是非谣言 output=0.71\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_list = [\n",
    "    '兴仁县今天抢小孩没抢走，把孩子母亲捅了一刀，看见这车的注意了，真事，车牌号辽HFM055！！！！！赶紧散播！ 都别带孩子出去瞎转悠了 尤其别让老人自己带孩子出去 太危险了 注意了！！！！辽HFM055北京现代朗动，在各学校门口抢小孩！！！110已经 证实！！全市通缉！！',\n",
    "    '重庆真实新闻:2016年6月1日在重庆梁平县袁驿镇发生一起抢儿童事件，做案人三个中年男人，在三中学校到镇街上的一条小路上，把小孩直接弄晕(儿童是袁驿新幼儿园中班的一名学生)，正准备带走时被家长及时发现用棒子赶走了做案人，故此获救！请各位同胞们以此引起非常重视，希望大家有爱心的人传递下',\n",
    "    '@尾熊C 要提前预习育儿知识的话，建议看一些小巫写的书，嘻嘻',\n",
    "]\n",
    "test_label=[0,0,1]\n",
    "for i in range(len(test_list)):\n",
    "    predict_rumor_LSTM(test_list[i],test_label[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=====================model1========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rumor_GRU(text,label):\n",
    "    print(text)\n",
    "    # 去标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # 分词\n",
    "    cut = seg.cut(text)\n",
    "    #去除停用词\n",
    "    cut_clean=[]\n",
    "    for word in cut:\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        cut_clean.append(word)\n",
    "    # tokenize\n",
    "    for i, word in enumerate(cut_clean):\n",
    "        try:\n",
    "            cut_clean[i] = cn_model.vocab[word].index\n",
    "            if cut_clean[i] >= 50000:\n",
    "                cut_clean[i] = 0\n",
    "        except KeyError:\n",
    "            cut_clean[i] = 0\n",
    "    # padding\n",
    "    tokens_pad = pad_sequences([cut_clean], maxlen=max_tokens,\n",
    "                           padding='pre', truncating='pre')\n",
    "    # 预测\n",
    "    dic={0:'谣言',1:'非谣言'}\n",
    "    result = model1.predict(x=tokens_pad)\n",
    "    coef = result[0][0]\n",
    "    if coef >= 0.5:\n",
    "        print('真实是'+dic[label],'预测是非谣言','output=%.2f'%coef)\n",
    "    else:\n",
    "        print('真实是'+dic[label],'预测是谣言','output=%.2f'%coef)\n",
    "    print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "兴仁县今天抢小孩没抢走，把孩子母亲捅了一刀，看见这车的注意了，真事，车牌号辽HFM055！！！！！赶紧散播！ 都别带孩子出去瞎转悠了 尤其别让老人自己带孩子出去 太危险了 注意了！！！！辽HFM055北京现代朗动，在各学校门口抢小孩！！！110已经 证实！！全市通缉！！\n",
      "真实是谣言 预测是谣言 output=0.10\n",
      "---------------------------------------------\n",
      "重庆真实新闻:2016年6月1日在重庆梁平县袁驿镇发生一起抢儿童事件，做案人三个中年男人，在三中学校到镇街上的一条小路上，把小孩直接弄晕(儿童是袁驿新幼儿园中班的一名学生)，正准备带走时被家长及时发现用棒子赶走了做案人，故此获救！请各位同胞们以此引起非常重视，希望大家有爱心的人传递下\n",
      "真实是谣言 预测是谣言 output=0.11\n",
      "---------------------------------------------\n",
      "@尾熊C 要提前预习育儿知识的话，建议看一些小巫写的书，嘻嘻\n",
      "真实是非谣言 预测是非谣言 output=0.61\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_list = [\n",
    "    '兴仁县今天抢小孩没抢走，把孩子母亲捅了一刀，看见这车的注意了，真事，车牌号辽HFM055！！！！！赶紧散播！ 都别带孩子出去瞎转悠了 尤其别让老人自己带孩子出去 太危险了 注意了！！！！辽HFM055北京现代朗动，在各学校门口抢小孩！！！110已经 证实！！全市通缉！！',\n",
    "    '重庆真实新闻:2016年6月1日在重庆梁平县袁驿镇发生一起抢儿童事件，做案人三个中年男人，在三中学校到镇街上的一条小路上，把小孩直接弄晕(儿童是袁驿新幼儿园中班的一名学生)，正准备带走时被家长及时发现用棒子赶走了做案人，故此获救！请各位同胞们以此引起非常重视，希望大家有爱心的人传递下',\n",
    "    '@尾熊C 要提前预习育儿知识的话，建议看一些小巫写的书，嘻嘻',\n",
    "]\n",
    "test_label=[0,0,1]\n",
    "for i in range(len(test_list)):\n",
    "    predict_rumor_GRU(test_list[i],test_label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TF2.1]",
   "language": "python",
   "name": "conda-env-TF2.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
