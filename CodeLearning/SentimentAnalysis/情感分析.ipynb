{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ä¸­æ–‡è¯å‘é‡ä¸‹è½½åœ°å€ï¼šhttps://github.com/Embedding/Chinese-Word-Vectors\n",
    "* æ•°æ®é›†ï¼šhttps://pan.baidu.com/s/1oObY4A_Ovo1CY00UrgbBKg æå–ç ï¼škth7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "# é­”æ³•å‘½ä»¤ï¼Œä½¿ç”¨åç”»å›¾ä¸ç”¨showäº†\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re# å¼•å…¥æ­£åˆ™\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.è§£å‹è¯å‘é‡å¹¶åŠ è½½"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1è§£å‹è¯å‘é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2# ç”¨æ¥è§£å‹æ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/embeddings/sgns.zhihu.bigram\", 'wb') as new_file, open(\"./data/embeddings/sgns.zhihu.bigram.bz2\", 'rb') as file:\n",
    "    decompressor = bz2.BZ2Decompressor()\n",
    "    for data in iter(lambda : file.read(100 * 1024), b''):\n",
    "        new_file.write(decompressor.decompress(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2åŠ è½½è¯å‘é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors# gensimç”¨æ¥åŠ è½½é¢„è®­ç»ƒè¯å‘é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_model = KeyedVectors.load_word2vec_format('./data/embeddings/sgns.zhihu.bigram', \n",
    "                                             binary=False,\n",
    "                                             unicode_errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.è¯­æ–™é¢„å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1è¯»å–åŸå§‹æ–‡æœ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pos_path='./data/4000/pos'\n",
    "neg_path='./data/4000/neg'\n",
    "\n",
    "pos_txts = os.listdir(pos_path)#posæ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰æ–‡ä»¶çš„åç§°\n",
    "neg_txts = os.listdir(neg_path)#negæ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰æ–‡ä»¶çš„åç§°\n",
    "\n",
    "train_texts_orig = []\n",
    "train_target = []\n",
    "\n",
    "for pos_txt in pos_txts:\n",
    "    if not os.path.isdir(pos_txt):#åˆ¤æ–­æ˜¯å¦æ˜¯æ–‡ä»¶å¤¹\n",
    "        with open(pos_path+'/'+pos_txt, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines=f.readlines()\n",
    "            train_texts_orig.append(''.join(lines))\n",
    "            train_target.append(1)\n",
    "            \n",
    "for neg_txt in neg_txts:\n",
    "    if not os.path.isdir(neg_txt):#åˆ¤æ–­æ˜¯å¦æ˜¯æ–‡ä»¶å¤¹\n",
    "        with open(neg_path+'/'+neg_txt, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines=f.readlines()\n",
    "            train_texts_orig.append(''.join(lines))\n",
    "            train_target.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ—©é¤å¤ªå·®ï¼Œæ— è®ºå»å¤šå°‘äººï¼Œé‚£è¾¹ä¹Ÿä¸åŠ é£Ÿå“çš„ã€‚é…’åº—åº”è¯¥é‡è§†ä¸€ä¸‹è¿™ä¸ªé—®é¢˜äº†ã€‚\n",
      "\n",
      "æˆ¿é—´æœ¬èº«å¾ˆå¥½ã€‚\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_texts_orig[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ—©é¤å¤ªå·®æ— è®ºå»å¤šå°‘äººé‚£è¾¹ä¹Ÿä¸åŠ é£Ÿå“çš„é…’åº—åº”è¯¥é‡è§†ä¸€ä¸‹è¿™ä¸ªé—®é¢˜äº†æˆ¿é—´æœ¬èº«å¾ˆå¥½\n"
     ]
    }
   ],
   "source": [
    "re_text=re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+â€”â€”ï¼ï¼Œã€‚ï¼Ÿã€~@#ï¿¥%â€¦â€¦&*ï¼ˆï¼‰]+\", \"\",train_texts_orig[2])\n",
    "print(re_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(train_target[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2è¿›è¡Œåˆ†è¯å’Œtokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. å»æ‰æ¯ä¸ªæ ·æœ¬çš„æ ‡ç‚¹ç¬¦å·ï¼›\n",
    "2. ç”¨jiebaåˆ†è¯ï¼Œå¾—åˆ°å­˜æ”¾åˆ†è¯ç»“æœçš„listâ€”cut_list\n",
    "3. å°†åˆ†è¯ç»“æœcut_listç´¢å¼•åŒ–ï¼Œè¿™æ ·æ¯ä¸€ä¾‹è¯„ä»·çš„æ–‡æœ¬å˜æˆä¸€æ®µç´¢å¼•æ•°å­—ï¼Œå¯¹åº”ç€é¢„è®­ç»ƒè¯å‘é‡æ¨¡å‹ä¸­çš„è¯ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train_tokensï¼šlist of listï¼Œlistä¸­å«æœ‰4000ä¸ªå°listï¼Œå¯¹åº”æ¯ä¸€æ¡è¯„ä»·åˆ†è¯ç´¢å¼•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba # ç»“å·´åˆ†è¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\dell\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 4.725 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "train_tokens = []\n",
    "for text in train_texts_orig:\n",
    "    # å»æ‰æ ‡ç‚¹\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+â€”â€”ï¼ï¼Œã€‚ï¼Ÿã€~@#ï¿¥%â€¦â€¦&*ï¼ˆï¼‰]+\", \"\",text)\n",
    "    # ç»“å·´åˆ†è¯\n",
    "    cut_list = jieba.lcut(text)\n",
    "\n",
    "    for i, word in enumerate(cut_list): # enumerate()\n",
    "        try:\n",
    "            # å°†è¯è½¬æ¢ä¸ºç´¢å¼•index\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "        except KeyError:\n",
    "            # å¦‚æœè¯ä¸åœ¨å­—å…¸ä¸­ï¼Œåˆ™è¾“å‡º0\n",
    "            cut_list[i] = 0\n",
    "    train_tokens.append(cut_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3ç´¢å¼•é•¿åº¦æ ‡å‡†åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å› ä¸ºæ¯æ®µè¯„è¯­çš„é•¿åº¦æ˜¯ä¸ä¸€æ ·çš„ï¼Œå¦‚æœå•çº¯å–æœ€é•¿çš„ä¸€ä¸ªè¯„è¯­ï¼Œå¹¶æŠŠå…¶ä»–è¯„å¡«å……æˆåŒæ ·çš„é•¿åº¦ï¼Œè¿™æ ·ååˆ†æµªè´¹è®¡ç®—èµ„æºï¼Œæ‰€ä»¥å–ä¸€ä¸ªæŠ˜è¡·çš„é•¿åº¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è·å¾—æ‰€æœ‰tokensçš„é•¿åº¦\n",
    "num_tokens = [len(tokens) for tokens in train_tokens]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# å–tokenså¹³å‡å€¼å¹¶åŠ ä¸Šä¸¤ä¸ªtokensçš„æ ‡å‡†å·®ï¼Œ\n",
    "# å‡è®¾tokensé•¿åº¦çš„åˆ†å¸ƒä¸ºæ­£æ€åˆ†å¸ƒï¼Œåˆ™max_tokensè¿™ä¸ªå€¼å¯ä»¥æ¶µç›–95%å·¦å³çš„æ ·æœ¬\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### paddingï¼ˆå¡«å……ï¼‰å’Œtruncatingï¼ˆä¿®å‰ªï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬æŠŠæ–‡æœ¬è½¬æ¢ä¸ºtokensï¼ˆç´¢å¼•ï¼‰ä¹‹åï¼Œæ¯ä¸€ä¸²ç´¢å¼•çš„é•¿åº¦å¹¶ä¸ç›¸ç­‰ï¼Œæ‰€ä»¥ä¸ºäº†æ–¹ä¾¿æ¨¡å‹çš„è®­ç»ƒæˆ‘ä»¬éœ€è¦æŠŠç´¢å¼•çš„é•¿åº¦æ ‡å‡†åŒ–ï¼Œä¸Šé¢æˆ‘ä»¬é€‰æ‹©äº†236è¿™ä¸ªå¯ä»¥æ¶µç›–95%è®­ç»ƒæ ·æœ¬çš„é•¿åº¦ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬è¿›è¡Œpaddingå’Œtruncatingï¼Œæˆ‘ä»¬ä¸€èˆ¬é‡‡ç”¨'pre'çš„æ–¹æ³•ï¼Œè¿™ä¼šåœ¨æ–‡æœ¬ç´¢å¼•çš„å‰é¢å¡«å……0ï¼Œå› ä¸ºæ ¹æ®ä¸€äº›ç ”ç©¶èµ„æ–™ä¸­çš„å®è·µï¼Œå¦‚æœåœ¨æ–‡æœ¬ç´¢å¼•åé¢å¡«å……0çš„è¯ï¼Œä¼šå¯¹æ¨¡å‹é€ æˆä¸€äº›ä¸è‰¯å½±å“ã€‚ \n",
    "\n",
    "è¿›è¡Œpaddingå’Œtruncatingï¼Œ è¾“å…¥çš„train_tokensæ˜¯ä¸€ä¸ªlist\n",
    "è¿”å›çš„train_padæ˜¯ä¸€ä¸ªnumpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pad = pad_sequences(train_tokens, maxlen=max_tokens,\n",
    "                            padding='pre', truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_pad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4å‡†å¤‡Embedding Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨æ¥ä¸ºæ¨¡å‹å‡†å¤‡embedding matrixï¼ˆè¯å‘é‡çŸ©é˜µï¼‰ï¼Œæ ¹æ®kerasçš„è¦æ±‚ï¼Œéœ€è¦å‡†å¤‡ä¸€ä¸ªç»´åº¦ä¸º (ğ‘›ğ‘¢ğ‘šğ‘¤ğ‘œğ‘Ÿğ‘‘ğ‘ ,ğ‘’ğ‘šğ‘ğ‘’ğ‘‘ğ‘‘ğ‘–ğ‘›ğ‘”ğ‘‘ğ‘–ğ‘š) çš„çŸ©é˜µï¼Œnum wordsä»£è¡¨ä½¿ç”¨çš„è¯æ±‡çš„æ•°é‡ï¼Œemdedding dimensionåœ¨ç°åœ¨ä½¿ç”¨çš„é¢„è®­ç»ƒè¯å‘é‡æ¨¡å‹ä¸­æ˜¯300ï¼Œæ¯ä¸€ä¸ªè¯æ±‡éƒ½ç”¨ä¸€ä¸ªé•¿åº¦ä¸º300çš„å‘é‡è¡¨ç¤ºã€‚\n",
    "æ³¨æ„åªé€‰æ‹©ä½¿ç”¨å‰50kä¸ªä½¿ç”¨é¢‘ç‡æœ€é«˜çš„è¯ï¼Œåœ¨è¿™ä¸ªé¢„è®­ç»ƒè¯å‘é‡æ¨¡å‹ä¸­ï¼Œä¸€å…±æœ‰260ä¸‡è¯æ±‡é‡ï¼Œå¦‚æœå…¨éƒ¨ä½¿ç”¨åœ¨åˆ†ç±»é—®é¢˜ä¸Šä¼šå¾ˆæµªè´¹è®¡ç®—èµ„æºï¼Œå› ä¸ºè®­ç»ƒæ ·æœ¬å¾ˆå°ï¼Œä¸€å…±åªæœ‰4kï¼Œå¦‚æœæœ‰100kï¼Œ200kç”šè‡³æ›´å¤šçš„è®­ç»ƒæ ·æœ¬æ—¶ï¼Œåœ¨åˆ†ç±»é—®é¢˜ä¸Šå¯ä»¥è€ƒè™‘å‡å°‘ä½¿ç”¨çš„è¯æ±‡é‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words = 50000\n",
    "embedding_dim=300\n",
    "# åˆå§‹åŒ–embedding_matrixï¼Œä¹‹ååœ¨kerasä¸Šè¿›è¡Œåº”ç”¨\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "# embedding_matrixä¸ºä¸€ä¸ª [num_wordsï¼Œembedding_dim] çš„çŸ©é˜µ\n",
    "# ç»´åº¦ä¸º 50000 * 300\n",
    "for i in range(num_words):\n",
    "    embedding_matrix[i,:] = cn_model[cn_model.index2word[i]]\n",
    "embedding_matrix = embedding_matrix.astype('float32')\n",
    "# æ£€æŸ¥indexæ˜¯å¦å¯¹åº”ï¼Œ\n",
    "# è¾“å‡º300æ„ä¹‰ä¸ºé•¿åº¦ä¸º300çš„embeddingå‘é‡ä¸€ä¸€å¯¹åº”\n",
    "np.sum( cn_model[cn_model.index2word[333]] == embedding_matrix[333] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¶…å‡ºäº”ä¸‡ä¸ªè¯å‘é‡çš„è¯ç”¨0ä»£æ›¿\n",
    "train_pad[ train_pad>=num_words ] = 0\n",
    "\n",
    "# å‡†å¤‡targetå‘é‡ï¼Œå‰2000æ ·æœ¬ä¸º1ï¼Œå2000ä¸º0\n",
    "train_target = np.array(train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.è®­ç»ƒè¯­æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•æ ·æœ¬çš„åˆ†å‰²\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 90%çš„æ ·æœ¬ç”¨æ¥è®­ç»ƒï¼Œå‰©ä½™10%ç”¨æ¥æµ‹è¯•\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_pad,\n",
    "                                                    train_target,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2æ­å»ºç½‘ç»œç»“æ„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç”¨kerasæ­å»ºLSTMæ¨¡å‹ï¼Œæ¨¡å‹çš„ç¬¬ä¸€å±‚æ˜¯Embeddingå±‚ï¼Œåªæœ‰å½“æˆ‘ä»¬æŠŠtokensç´¢å¼•è½¬æ¢ä¸ºè¯å‘é‡çŸ©é˜µä¹‹åï¼Œæ‰å¯ä»¥ç”¨ç¥ç»ç½‘ç»œå¯¹æ–‡æœ¬è¿›è¡Œå¤„ç†ã€‚ kerasæä¾›äº†Embeddingæ¥å£ï¼Œé¿å…äº†ç¹ççš„ç¨€ç–çŸ©é˜µæ“ä½œã€‚åœ¨Embeddingå±‚æˆ‘ä»¬è¾“å…¥çš„çŸ©é˜µä¸ºï¼š(ğ‘ğ‘ğ‘¡ğ‘â„ğ‘ ğ‘–ğ‘§ğ‘’,ğ‘šğ‘ğ‘¥ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘ )è¾“å‡ºçŸ©é˜µä¸º:(ğ‘ğ‘ğ‘¡ğ‘â„ğ‘ ğ‘–ğ‘§ğ‘’,ğ‘šğ‘ğ‘¥ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘ ,ğ‘’ğ‘šğ‘ğ‘’ğ‘‘ğ‘‘ğ‘–ğ‘›ğ‘”ğ‘‘ğ‘–ğ‘š)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_tokens,\n",
    "                    trainable=False))\n",
    "model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "model.add(LSTM(units=16, return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3æ¨¡å‹é…ç½®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ¨¡å‹ä¿å­˜ï¼ˆæ–­ç‚¹ç»­è®­ï¼‰ã€early stopingã€å­¦ä¹ ç‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹ä¸€ä¸ªæƒé‡çš„å­˜å‚¨ç‚¹\n",
    "path_checkpoint = './checkpoint/sentiment_checkpoint.keras'\n",
    "checkpoint = ModelCheckpoint(filepath=path_checkpoint, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°è¯•åŠ è½½å·²è®­ç»ƒæ¨¡å‹\n",
    "try:\n",
    "    model.load_weights(path_checkpoint)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰early stopingå¦‚æœ3ä¸ªepochå†…validation lossæ²¡æœ‰æ”¹å–„åˆ™åœæ­¢è®­ç»ƒ\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "# è‡ªåŠ¨é™ä½learning rate\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1, min_lr=1e-8, patience=0,\n",
    "                                       verbose=1)\n",
    "# å®šä¹‰callbackå‡½æ•°\n",
    "callbacks = [\n",
    "    earlystopping, \n",
    "#    checkpoint,\n",
    "    lr_reduction\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4è®­ç»ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3240 samples, validate on 360 samples\n",
      "Epoch 1/20\n",
      "3240/3240 [==============================] - 137s 42ms/sample - loss: 0.5870 - accuracy: 0.7019 - val_loss: 0.5294 - val_accuracy: 0.7444\n",
      "Epoch 2/20\n",
      "3240/3240 [==============================] - 115s 36ms/sample - loss: 0.4264 - accuracy: 0.8154 - val_loss: 0.3796 - val_accuracy: 0.8417\n",
      "Epoch 3/20\n",
      "3240/3240 [==============================] - 103s 32ms/sample - loss: 0.3456 - accuracy: 0.8670 - val_loss: 0.3539 - val_accuracy: 0.8694\n",
      "Epoch 4/20\n",
      "3240/3240 [==============================] - 136s 42ms/sample - loss: 0.2955 - accuracy: 0.8846 - val_loss: 0.3215 - val_accuracy: 0.8722\n",
      "Epoch 5/20\n",
      "3240/3240 [==============================] - 137s 42ms/sample - loss: 0.2675 - accuracy: 0.9009 - val_loss: 0.2888 - val_accuracy: 0.8917\n",
      "Epoch 6/20\n",
      "3240/3240 [==============================] - 130s 40ms/sample - loss: 0.2429 - accuracy: 0.9108 - val_loss: 0.3560 - val_accuracy: 0.8556\n",
      "Epoch 7/20\n",
      "3240/3240 [==============================] - 123s 38ms/sample - loss: 0.2400 - accuracy: 0.9139 - val_loss: 0.3321 - val_accuracy: 0.8583\n",
      "Epoch 8/20\n",
      "3240/3240 [==============================] - 114s 35ms/sample - loss: 0.2179 - accuracy: 0.9160 - val_loss: 0.3171 - val_accuracy: 0.8889\n",
      "Epoch 9/20\n",
      "3240/3240 [==============================] - 134s 41ms/sample - loss: 0.1912 - accuracy: 0.9327 - val_loss: 0.3130 - val_accuracy: 0.8667\n",
      "Epoch 10/20\n",
      "3240/3240 [==============================] - 154s 48ms/sample - loss: 0.1647 - accuracy: 0.9410 - val_loss: 0.3200 - val_accuracy: 0.8833\n",
      "Epoch 11/20\n",
      "3240/3240 [==============================] - 101s 31ms/sample - loss: 0.1384 - accuracy: 0.9540 - val_loss: 0.3041 - val_accuracy: 0.8944\n",
      "Epoch 12/20\n",
      "3240/3240 [==============================] - 106s 33ms/sample - loss: 0.1621 - accuracy: 0.9454 - val_loss: 0.3116 - val_accuracy: 0.8944\n",
      "Epoch 13/20\n",
      "3240/3240 [==============================] - 124s 38ms/sample - loss: 0.1242 - accuracy: 0.9636 - val_loss: 0.3197 - val_accuracy: 0.8944\n",
      "Epoch 14/20\n",
      "3240/3240 [==============================] - 141s 44ms/sample - loss: 0.1173 - accuracy: 0.9630 - val_loss: 0.3624 - val_accuracy: 0.8833\n",
      "Epoch 15/20\n",
      "3240/3240 [==============================] - 199s 61ms/sample - loss: 0.2177 - accuracy: 0.9188 - val_loss: 0.2968 - val_accuracy: 0.8833\n",
      "Epoch 16/20\n",
      "3240/3240 [==============================] - 148s 46ms/sample - loss: 0.1193 - accuracy: 0.9614 - val_loss: 0.2860 - val_accuracy: 0.9111\n",
      "Epoch 17/20\n",
      "3240/3240 [==============================] - 179s 55ms/sample - loss: 0.0889 - accuracy: 0.9775 - val_loss: 0.2953 - val_accuracy: 0.9111\n",
      "Epoch 18/20\n",
      "3240/3240 [==============================] - 177s 55ms/sample - loss: 0.0783 - accuracy: 0.9784 - val_loss: 0.4399 - val_accuracy: 0.8639\n",
      "Epoch 19/20\n",
      "3240/3240 [==============================] - 177s 55ms/sample - loss: 0.0958 - accuracy: 0.9701 - val_loss: 0.3708 - val_accuracy: 0.8972\n",
      "Epoch 20/20\n",
      "3240/3240 [==============================] - 174s 54ms/sample - loss: 0.0751 - accuracy: 0.9778 - val_loss: 0.3376 - val_accuracy: 0.9056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xcc89a8dc08>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# å¼€å§‹è®­ç»ƒ\n",
    "model.fit(X_train, y_train,validation_split=0.1,epochs=20,batch_size=128)#,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5åº”ç”¨äºæµ‹è¯•é›†\n",
    "é¦–å…ˆå¯¹æµ‹è¯•æ ·æœ¬è¿›è¡Œé¢„æµ‹ï¼Œå¾—åˆ°äº†è¿˜ç®—æ»¡æ„çš„å‡†ç¡®åº¦ã€‚ä¹‹åæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªé¢„æµ‹å‡½æ•°ï¼Œæ¥é¢„æµ‹è¾“å…¥çš„æ–‡æœ¬çš„ææ€§ï¼Œå¯è§æ¨¡å‹å¯¹äºå¦å®šå¥å’Œä¸€äº›ç®€å•çš„é€»è¾‘ç»“æ„éƒ½å¯ä»¥è¿›è¡Œå‡†ç¡®çš„åˆ¤æ–­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 6s 15ms/sample - loss: 0.3737 - accuracy: 0.8825\n",
      "Accuracy:88.25%\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.é¢„æµ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    print(text)\n",
    "    # å»æ ‡ç‚¹\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+â€”â€”ï¼ï¼Œã€‚ï¼Ÿã€~@#ï¿¥%â€¦â€¦&*ï¼ˆï¼‰]+\", \"\",text)\n",
    "    # åˆ†è¯\n",
    "    cut = jieba.cut(text)\n",
    "    cut_list = [ i for i in cut ]\n",
    "    # tokenize\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "            if cut_list[i] >= 50000:\n",
    "                cut_list[i] = 0\n",
    "        except KeyError:\n",
    "            cut_list[i] = 0\n",
    "    # padding\n",
    "    tokens_pad = pad_sequences([cut_list], maxlen=max_tokens,\n",
    "                           padding='pre', truncating='pre')\n",
    "    # é¢„æµ‹\n",
    "    result = model.predict(x=tokens_pad)\n",
    "    coef = result[0][0]\n",
    "    if coef >= 0.5:\n",
    "        print('æ˜¯ä¸€ä¾‹æ­£é¢è¯„ä»·','output=%.2f'%coef)\n",
    "    else:\n",
    "        print('æ˜¯ä¸€ä¾‹è´Ÿé¢è¯„ä»·','output=%.2f'%coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é…’åº—è®¾æ–½ä¸æ˜¯æ–°çš„ï¼ŒæœåŠ¡æ€åº¦å¾ˆä¸å¥½\n",
      "æ˜¯ä¸€ä¾‹è´Ÿé¢è¯„ä»· output=0.05\n",
      "é…’åº—å«ç”Ÿæ¡ä»¶éå¸¸ä¸å¥½\n",
      "æ˜¯ä¸€ä¾‹è´Ÿé¢è¯„ä»· output=0.03\n",
      "åºŠé“ºéå¸¸èˆ’é€‚\n",
      "æ˜¯ä¸€ä¾‹æ­£é¢è¯„ä»· output=0.92\n",
      "æˆ¿é—´å¾ˆå‡‰ï¼Œä¸ç»™å¼€æš–æ°”\n",
      "æ˜¯ä¸€ä¾‹è´Ÿé¢è¯„ä»· output=0.28\n",
      "æˆ¿é—´å¾ˆå‡‰çˆ½ï¼Œç©ºè°ƒå†·æ°”å¾ˆè¶³\n",
      "æ˜¯ä¸€ä¾‹æ­£é¢è¯„ä»· output=0.66\n",
      "é…’åº—ç¯å¢ƒä¸å¥½ï¼Œä½å®¿ä½“éªŒå¾ˆä¸å¥½\n",
      "æ˜¯ä¸€ä¾‹è´Ÿé¢è¯„ä»· output=0.02\n",
      "æˆ¿é—´éš”éŸ³ä¸åˆ°ä½\n",
      "æ˜¯ä¸€ä¾‹è´Ÿé¢è¯„ä»· output=0.25\n",
      "æ™šä¸Šå›æ¥å‘ç°æ²¡æœ‰æ‰“æ‰«å«ç”Ÿ\n",
      "æ˜¯ä¸€ä¾‹è´Ÿé¢è¯„ä»· output=0.09\n",
      "å› ä¸ºè¿‡èŠ‚æ‰€ä»¥è¦æˆ‘ä¸´æ—¶åŠ é’±ï¼Œæ¯”å›¢è´­çš„ä»·æ ¼è´µ\n",
      "æ˜¯ä¸€ä¾‹è´Ÿé¢è¯„ä»· output=0.02\n"
     ]
    }
   ],
   "source": [
    "test_list = [\n",
    "    'é…’åº—è®¾æ–½ä¸æ˜¯æ–°çš„ï¼ŒæœåŠ¡æ€åº¦å¾ˆä¸å¥½',\n",
    "    'é…’åº—å«ç”Ÿæ¡ä»¶éå¸¸ä¸å¥½',\n",
    "    'åºŠé“ºéå¸¸èˆ’é€‚',\n",
    "    'æˆ¿é—´å¾ˆå‡‰ï¼Œä¸ç»™å¼€æš–æ°”',\n",
    "    'æˆ¿é—´å¾ˆå‡‰çˆ½ï¼Œç©ºè°ƒå†·æ°”å¾ˆè¶³',\n",
    "    'é…’åº—ç¯å¢ƒä¸å¥½ï¼Œä½å®¿ä½“éªŒå¾ˆä¸å¥½',\n",
    "    'æˆ¿é—´éš”éŸ³ä¸åˆ°ä½' ,\n",
    "    'æ™šä¸Šå›æ¥å‘ç°æ²¡æœ‰æ‰“æ‰«å«ç”Ÿ',\n",
    "    'å› ä¸ºè¿‡èŠ‚æ‰€ä»¥è¦æˆ‘ä¸´æ—¶åŠ é’±ï¼Œæ¯”å›¢è´­çš„ä»·æ ¼è´µ'\n",
    "]\n",
    "for text in test_list:\n",
    "    predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TF2.1]",
   "language": "python",
   "name": "conda-env-TF2.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
