{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 中文词向量下载地址：https://github.com/Embedding/Chinese-Word-Vectors\n",
    "* 数据集：https://pan.baidu.com/s/1oObY4A_Ovo1CY00UrgbBKg 提取码：kth7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "# 魔法命令，使用后画图不用show了\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re# 引入正则\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.解压词向量并加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1解压词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2# 用来解压文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/embeddings/sgns.zhihu.bigram\", 'wb') as new_file, open(\"./data/embeddings/sgns.zhihu.bigram.bz2\", 'rb') as file:\n",
    "    decompressor = bz2.BZ2Decompressor()\n",
    "    for data in iter(lambda : file.read(100 * 1024), b''):\n",
    "        new_file.write(decompressor.decompress(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2加载词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors# gensim用来加载预训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_model = KeyedVectors.load_word2vec_format('./data/embeddings/sgns.zhihu.bigram', \n",
    "                                             binary=False,\n",
    "                                             unicode_errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.语料预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1读取原始文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pos_path='./data/4000/pos'\n",
    "neg_path='./data/4000/neg'\n",
    "\n",
    "pos_txts = os.listdir(pos_path)#pos文件夹下所有文件的名称\n",
    "neg_txts = os.listdir(neg_path)#neg文件夹下所有文件的名称\n",
    "\n",
    "train_texts_orig = []\n",
    "train_target = []\n",
    "\n",
    "for pos_txt in pos_txts:\n",
    "    if not os.path.isdir(pos_txt):#判断是否是文件夹\n",
    "        with open(pos_path+'/'+pos_txt, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines=f.readlines()\n",
    "            train_texts_orig.append(''.join(lines))\n",
    "            train_target.append(1)\n",
    "            \n",
    "for neg_txt in neg_txts:\n",
    "    if not os.path.isdir(neg_txt):#判断是否是文件夹\n",
    "        with open(neg_path+'/'+neg_txt, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines=f.readlines()\n",
    "            train_texts_orig.append(''.join(lines))\n",
    "            train_target.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。\n",
      "\n",
      "房间本身很好。\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_texts_orig[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "早餐太差无论去多少人那边也不加食品的酒店应该重视一下这个问题了房间本身很好\n"
     ]
    }
   ],
   "source": [
    "re_text=re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",train_texts_orig[2])\n",
    "print(re_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(train_target[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2进行分词和tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 去掉每个样本的标点符号；\n",
    "2. 用jieba分词，得到存放分词结果的list—cut_list\n",
    "3. 将分词结果cut_list索引化，这样每一例评价的文本变成一段索引数字，对应着预训练词向量模型中的词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train_tokens：list of list，list中含有4000个小list，对应每一条评价分词索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba # 结巴分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\dell\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 4.725 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "train_tokens = []\n",
    "for text in train_texts_orig:\n",
    "    # 去掉标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # 结巴分词\n",
    "    cut_list = jieba.lcut(text)\n",
    "\n",
    "    for i, word in enumerate(cut_list): # enumerate()\n",
    "        try:\n",
    "            # 将词转换为索引index\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "        except KeyError:\n",
    "            # 如果词不在字典中，则输出0\n",
    "            cut_list[i] = 0\n",
    "    train_tokens.append(cut_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3索引长度标准化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为每段评语的长度是不一样的，如果单纯取最长的一个评语，并把其他评填充成同样的长度，这样十分浪费计算资源，所以取一个折衷的长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获得所有tokens的长度\n",
    "num_tokens = [len(tokens) for tokens in train_tokens]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 取tokens平均值并加上两个tokens的标准差，\n",
    "# 假设tokens长度的分布为正态分布，则max_tokens这个值可以涵盖95%左右的样本\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### padding（填充）和truncating（修剪）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们把文本转换为tokens（索引）之后，每一串索引的长度并不相等，所以为了方便模型的训练我们需要把索引的长度标准化，上面我们选择了236这个可以涵盖95%训练样本的长度，接下来我们进行padding和truncating，我们一般采用'pre'的方法，这会在文本索引的前面填充0，因为根据一些研究资料中的实践，如果在文本索引后面填充0的话，会对模型造成一些不良影响。 \n",
    "\n",
    "进行padding和truncating， 输入的train_tokens是一个list\n",
    "返回的train_pad是一个numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pad = pad_sequences(train_tokens, maxlen=max_tokens,\n",
    "                            padding='pre', truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_pad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4准备Embedding Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在来为模型准备embedding matrix（词向量矩阵），根据keras的要求，需要准备一个维度为 (𝑛𝑢𝑚𝑤𝑜𝑟𝑑𝑠,𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔𝑑𝑖𝑚) 的矩阵，num words代表使用的词汇的数量，emdedding dimension在现在使用的预训练词向量模型中是300，每一个词汇都用一个长度为300的向量表示。\n",
    "注意只选择使用前50k个使用频率最高的词，在这个预训练词向量模型中，一共有260万词汇量，如果全部使用在分类问题上会很浪费计算资源，因为训练样本很小，一共只有4k，如果有100k，200k甚至更多的训练样本时，在分类问题上可以考虑减少使用的词汇量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words = 50000\n",
    "embedding_dim=300\n",
    "# 初始化embedding_matrix，之后在keras上进行应用\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "# embedding_matrix为一个 [num_words，embedding_dim] 的矩阵\n",
    "# 维度为 50000 * 300\n",
    "for i in range(num_words):\n",
    "    embedding_matrix[i,:] = cn_model[cn_model.index2word[i]]\n",
    "embedding_matrix = embedding_matrix.astype('float32')\n",
    "# 检查index是否对应，\n",
    "# 输出300意义为长度为300的embedding向量一一对应\n",
    "np.sum( cn_model[cn_model.index2word[333]] == embedding_matrix[333] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超出五万个词向量的词用0代替\n",
    "train_pad[ train_pad>=num_words ] = 0\n",
    "\n",
    "# 准备target向量，前2000样本为1，后2000为0\n",
    "train_target = np.array(train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.训练语料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1划分训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行训练和测试样本的分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 90%的样本用来训练，剩余10%用来测试\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_pad,\n",
    "                                                    train_target,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2搭建网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用keras搭建LSTM模型，模型的第一层是Embedding层，只有当我们把tokens索引转换为词向量矩阵之后，才可以用神经网络对文本进行处理。 keras提供了Embedding接口，避免了繁琐的稀疏矩阵操作。在Embedding层我们输入的矩阵为：(𝑏𝑎𝑡𝑐ℎ𝑠𝑖𝑧𝑒,𝑚𝑎𝑥𝑡𝑜𝑘𝑒𝑛𝑠)输出矩阵为:(𝑏𝑎𝑡𝑐ℎ𝑠𝑖𝑧𝑒,𝑚𝑎𝑥𝑡𝑜𝑘𝑒𝑛𝑠,𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔𝑑𝑖𝑚)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_tokens,\n",
    "                    trainable=False))\n",
    "model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "model.add(LSTM(units=16, return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3模型配置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型保存（断点续训）、early stoping、学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立一个权重的存储点\n",
    "path_checkpoint = './checkpoint/sentiment_checkpoint.keras'\n",
    "checkpoint = ModelCheckpoint(filepath=path_checkpoint, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尝试加载已训练模型\n",
    "try:\n",
    "    model.load_weights(path_checkpoint)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义early stoping如果3个epoch内validation loss没有改善则停止训练\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "# 自动降低learning rate\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1, min_lr=1e-8, patience=0,\n",
    "                                       verbose=1)\n",
    "# 定义callback函数\n",
    "callbacks = [\n",
    "    earlystopping, \n",
    "#    checkpoint,\n",
    "    lr_reduction\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3240 samples, validate on 360 samples\n",
      "Epoch 1/20\n",
      "3240/3240 [==============================] - 137s 42ms/sample - loss: 0.5870 - accuracy: 0.7019 - val_loss: 0.5294 - val_accuracy: 0.7444\n",
      "Epoch 2/20\n",
      "3240/3240 [==============================] - 115s 36ms/sample - loss: 0.4264 - accuracy: 0.8154 - val_loss: 0.3796 - val_accuracy: 0.8417\n",
      "Epoch 3/20\n",
      "3240/3240 [==============================] - 103s 32ms/sample - loss: 0.3456 - accuracy: 0.8670 - val_loss: 0.3539 - val_accuracy: 0.8694\n",
      "Epoch 4/20\n",
      "3240/3240 [==============================] - 136s 42ms/sample - loss: 0.2955 - accuracy: 0.8846 - val_loss: 0.3215 - val_accuracy: 0.8722\n",
      "Epoch 5/20\n",
      "3240/3240 [==============================] - 137s 42ms/sample - loss: 0.2675 - accuracy: 0.9009 - val_loss: 0.2888 - val_accuracy: 0.8917\n",
      "Epoch 6/20\n",
      "3240/3240 [==============================] - 130s 40ms/sample - loss: 0.2429 - accuracy: 0.9108 - val_loss: 0.3560 - val_accuracy: 0.8556\n",
      "Epoch 7/20\n",
      "3240/3240 [==============================] - 123s 38ms/sample - loss: 0.2400 - accuracy: 0.9139 - val_loss: 0.3321 - val_accuracy: 0.8583\n",
      "Epoch 8/20\n",
      "3240/3240 [==============================] - 114s 35ms/sample - loss: 0.2179 - accuracy: 0.9160 - val_loss: 0.3171 - val_accuracy: 0.8889\n",
      "Epoch 9/20\n",
      "3240/3240 [==============================] - 134s 41ms/sample - loss: 0.1912 - accuracy: 0.9327 - val_loss: 0.3130 - val_accuracy: 0.8667\n",
      "Epoch 10/20\n",
      "3240/3240 [==============================] - 154s 48ms/sample - loss: 0.1647 - accuracy: 0.9410 - val_loss: 0.3200 - val_accuracy: 0.8833\n",
      "Epoch 11/20\n",
      "3240/3240 [==============================] - 101s 31ms/sample - loss: 0.1384 - accuracy: 0.9540 - val_loss: 0.3041 - val_accuracy: 0.8944\n",
      "Epoch 12/20\n",
      "3240/3240 [==============================] - 106s 33ms/sample - loss: 0.1621 - accuracy: 0.9454 - val_loss: 0.3116 - val_accuracy: 0.8944\n",
      "Epoch 13/20\n",
      "3240/3240 [==============================] - 124s 38ms/sample - loss: 0.1242 - accuracy: 0.9636 - val_loss: 0.3197 - val_accuracy: 0.8944\n",
      "Epoch 14/20\n",
      "3240/3240 [==============================] - 141s 44ms/sample - loss: 0.1173 - accuracy: 0.9630 - val_loss: 0.3624 - val_accuracy: 0.8833\n",
      "Epoch 15/20\n",
      "3240/3240 [==============================] - 199s 61ms/sample - loss: 0.2177 - accuracy: 0.9188 - val_loss: 0.2968 - val_accuracy: 0.8833\n",
      "Epoch 16/20\n",
      "3240/3240 [==============================] - 148s 46ms/sample - loss: 0.1193 - accuracy: 0.9614 - val_loss: 0.2860 - val_accuracy: 0.9111\n",
      "Epoch 17/20\n",
      "3240/3240 [==============================] - 179s 55ms/sample - loss: 0.0889 - accuracy: 0.9775 - val_loss: 0.2953 - val_accuracy: 0.9111\n",
      "Epoch 18/20\n",
      "3240/3240 [==============================] - 177s 55ms/sample - loss: 0.0783 - accuracy: 0.9784 - val_loss: 0.4399 - val_accuracy: 0.8639\n",
      "Epoch 19/20\n",
      "3240/3240 [==============================] - 177s 55ms/sample - loss: 0.0958 - accuracy: 0.9701 - val_loss: 0.3708 - val_accuracy: 0.8972\n",
      "Epoch 20/20\n",
      "3240/3240 [==============================] - 174s 54ms/sample - loss: 0.0751 - accuracy: 0.9778 - val_loss: 0.3376 - val_accuracy: 0.9056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xcc89a8dc08>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 开始训练\n",
    "model.fit(X_train, y_train,validation_split=0.1,epochs=20,batch_size=128)#,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5应用于测试集\n",
    "首先对测试样本进行预测，得到了还算满意的准确度。之后我们定义一个预测函数，来预测输入的文本的极性，可见模型对于否定句和一些简单的逻辑结构都可以进行准确的判断。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 6s 15ms/sample - loss: 0.3737 - accuracy: 0.8825\n",
      "Accuracy:88.25%\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    print(text)\n",
    "    # 去标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # 分词\n",
    "    cut = jieba.cut(text)\n",
    "    cut_list = [ i for i in cut ]\n",
    "    # tokenize\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "            if cut_list[i] >= 50000:\n",
    "                cut_list[i] = 0\n",
    "        except KeyError:\n",
    "            cut_list[i] = 0\n",
    "    # padding\n",
    "    tokens_pad = pad_sequences([cut_list], maxlen=max_tokens,\n",
    "                           padding='pre', truncating='pre')\n",
    "    # 预测\n",
    "    result = model.predict(x=tokens_pad)\n",
    "    coef = result[0][0]\n",
    "    if coef >= 0.5:\n",
    "        print('是一例正面评价','output=%.2f'%coef)\n",
    "    else:\n",
    "        print('是一例负面评价','output=%.2f'%coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "酒店设施不是新的，服务态度很不好\n",
      "是一例负面评价 output=0.05\n",
      "酒店卫生条件非常不好\n",
      "是一例负面评价 output=0.03\n",
      "床铺非常舒适\n",
      "是一例正面评价 output=0.92\n",
      "房间很凉，不给开暖气\n",
      "是一例负面评价 output=0.28\n",
      "房间很凉爽，空调冷气很足\n",
      "是一例正面评价 output=0.66\n",
      "酒店环境不好，住宿体验很不好\n",
      "是一例负面评价 output=0.02\n",
      "房间隔音不到位\n",
      "是一例负面评价 output=0.25\n",
      "晚上回来发现没有打扫卫生\n",
      "是一例负面评价 output=0.09\n",
      "因为过节所以要我临时加钱，比团购的价格贵\n",
      "是一例负面评价 output=0.02\n"
     ]
    }
   ],
   "source": [
    "test_list = [\n",
    "    '酒店设施不是新的，服务态度很不好',\n",
    "    '酒店卫生条件非常不好',\n",
    "    '床铺非常舒适',\n",
    "    '房间很凉，不给开暖气',\n",
    "    '房间很凉爽，空调冷气很足',\n",
    "    '酒店环境不好，住宿体验很不好',\n",
    "    '房间隔音不到位' ,\n",
    "    '晚上回来发现没有打扫卫生',\n",
    "    '因为过节所以要我临时加钱，比团购的价格贵'\n",
    "]\n",
    "for text in test_list:\n",
    "    predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TF2.1]",
   "language": "python",
   "name": "conda-env-TF2.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
